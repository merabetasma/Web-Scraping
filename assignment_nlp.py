# -*- coding: utf-8 -*-
"""assignment nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HBHv1JxghBjqsyRRdBW7MXCrlGGQ2RIZ

# Import libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.util import pr
from nltk.corpus import stopwords
import warnings
warnings.filterwarnings('ignore')

"""# Load  and read dataset"""

from google.colab import files
uploades=files.upload()
data=pd.read_excel('Input.xlsx')

"""# Data Extraction"""

import requests
from urllib.request import Request,urlopen
from bs4 import BeautifulSoup
urls= data['URL']
# target url
titles=[]
articles=[]
textt=''
for url in urls:
  try:
    req = Request(url)
    req.add_header('user-agent','Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36 OPR/91.0.4516.77')
    rawpage = urlopen(req).read()
    soup = BeautifulSoup(rawpage, 'html.parser')
    content = soup.article
    for title in soup.find_all('title'):
        
        titles.append(title.get_text())

    for text in soup.find_all("p"):
        
          textt+=(text.get_text())

    articles.append(textt)
    textt=''        
  except Exception:
    titles.append("No Found")
    articles.append("No Found")
    print(url)
    pass
     
data['title']=titles
data['text']=articles

data.head()

"""# Read and concatenate the stop words text files"""

import glob
path='/content/drive/MyDrive/20211030 Test Assignment/StopWords'
read_files = glob.glob(path+ "/*.txt")

with open("result.txt", "wb") as outfile:
    for f in read_files:
        with open(f, "rb") as infile:
            outfile.write(infile.read())

stopword=[]
with open('/content/result.txt','r',encoding='latin-1')as file:
    for item in file :
      stopword.append(item.rstrip())

"""# Read positive and negative words text files"""

positive_word=[]
with open('/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/positive-words.txt','r',encoding='latin-1')as file:
    for item in file :
      positive_word.append(item.rstrip())

negative_word=[]
with open('/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary/negative-words.txt','r',encoding='latin-1')as file:
    for item in file :
      negative_word.append(item.rstrip())

"""# Cleaning the text"""

def clean(text):
 

 #remove hyperlinks
 text = re.sub('https?://\S+|www\.\S+', '', text)
 #remove any character enter < > and < >
 text = re.sub('<.*?>+', '', text)
 #remove punctuation
 #text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
 #remove \n newline
 text = re.sub('\n', '', text)
 #remove symbols and digital(a9a)
 text = re.sub('\w*\d\w*', '', text)
 #stop word
 text = [word for word in text.split(' ') if word not in stopword]
 text=" ".join(text)
 #stemming
 #stemmer = nltk.SnowballStemmer("english")
 #text = [stemmer.stem(word) for word in text.split(' ')]
 #text=" ".join(text)
 #lemmatization
 return text
data["text"] = data["text"].apply(clean)

"""# 1-positive score"""

import nltk
from nltk import word_tokenize,sent_tokenize
nltk.download('punkt')
sp=[]
sn=[]
cword=[]
nw=0
n=0
p=0
for i  in data['text']:
    for w in (word_tokenize(i)):

       p+=positive_word.count(w)
       n+=negative_word.count(w)
       nw+=1

    cword.append(nw)
    sp.append(p)
    sn.append(n)
    p=0
    n=0
    nw=0
data['positive score']= sp

"""#2-Negative score"""

data['negative score']=sn

"""# 3-Polarity Score"""

data['Polarity Score'] =(data['positive score'] - data['negative score']) / ((data['positive score'] + data['negative score']) + 0.000001)

"""# 4-subjectivity score"""

data['subjectivity score'] = (data['positive score'] - data['negative score']) / ((data['text'].apply(lambda x: len( word_tokenize(x)))) + 0.000001)

"""# 5-Average Sentence Length"""

data['Average Sentence Length'] =  data["text"].apply(lambda x: np.mean([len(l) for l in  word_tokenize(x)]))

"""#6- wORDS COUNT"""

import string
data['clean_text']= data['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))
data['Total Number words']= data['clean_text'].apply(lambda x: len( word_tokenize(x)))

"""# 7-Average Number of Words Per Sentence"""

data[' Total Number sentences']= data['text'].apply(lambda x: len(sent_tokenize(x)))

data['Average Number of Words Per Sentence']=data['Total Number words']/data[' Total Number sentences']

"""#8-Complex word count"""

com_word=[]
count = 0
for i  in data['clean_text']:
 for w in (word_tokenize(i)):

    d = {}.fromkeys('aeiou',0) 
    haslotsvowels = False 
    for x in w.lower(): 
      if x in d: 
          d[x] += 1 
    for q in d.values(): 
          if q > 2: 
            haslotsvowels = True 
    if haslotsvowels: 
          count += 1 
 com_word.append(count)
data['complex word count']=com_word

"""#9- Percentage  Complex words"""

data['Percentage  Complex words'] = data['complex word count'] / data['Total Number words']

"""#10- Fog Index"""

data['Fog Index'] = 0.4 * (data['Average Sentence Length'] + data['Percentage  Complex words'])

"""#11- Syllable Count Per Word"""

syllb=[]
count = 0
vowels = 'aeiouy'
for i  in data['clean_text']:
    for word in (word_tokenize(i)):

        word = word.lower().strip(".:;?!")
        if word[0] in vowels:
           count +=1
        for index in range(1,len(word)):
            if word[index] in vowels and word[index-1] not in vowels:
                 count +=1
        if word.endswith('ed'):
            count -= 1
        if word.endswith('es'):
            count-=1
        if count == 0:
            count +=1
    syllb.append (count)

data['Syllable Count Per Word']=syllb

"""#12- Personal Pronouns"""

import re
pronounRegex = re.compile(r'\b(I|we|my|ours|(?-i:us))\b',re.I)
data['Personal Pronouns']= data['text'].apply(lambda x: len(pronounRegex.findall(x)))

"""# 13-Average Word Length"""

data['Average Word Length'] = data["clean_text"].apply(lambda x: np.mean([len(l) for l in  word_tokenize(x)]))

data.head()

data.to_excel('output.xlsx',index=False)